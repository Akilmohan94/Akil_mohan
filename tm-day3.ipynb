{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "- Web scraping\n",
    "- Text summarization\n",
    "- Topic Modeling\n",
    "- Named Entity Recognition\n",
    "- Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.monsterindia.com/data-science-jobs.html'\n",
    "page = requests.get(url).text\n",
    "type(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science Jobs - 118 Data Science Job Vacancies - Monster India'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = bs.findAll('div', {'class': 'jobwrap', 'type': 'tuple'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.DataFrame(columns=['title', 'company', 'location',\n",
    "                                'exp', 'posted_at', 'skills'])\n",
    "for job in jobs:\n",
    "    title = job.find('span', {'class': 'title_in'}).text\n",
    "    company = job.find('a', {'class': 'jtxt orange'}).get('title')\n",
    "    location = job.find('div', {'class': 'jtxt jico ico1'}).text\n",
    "    exp = job.find('div', {'class': 'jtxt jico ico2'}).text\n",
    "    posted_at = job.find('div', {'class': 'job_optitem ico7',\n",
    "                                 'itemprop':'datePosted'}).text\n",
    "    skills = job.find('div', {'class': 'joblnk serachjoblnk'}).findAll('div')[4].get('title')\n",
    "    #print(skills)\n",
    "    curr_job = {'title': title,\n",
    "                'company': company,\n",
    "                'location': location,\n",
    "                'experience': exp,\n",
    "                'posted_at': posted_at,\n",
    "                'skills': skills\n",
    "               }\n",
    "    jobs_df = jobs_df.append(curr_job, ignore_index=True)\n",
    "jobs_df.to_csv('monster.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.monsterindia.com/data-science-jobs-1.html\n",
      "https://www.monsterindia.com/data-science-jobs-2.html\n",
      "https://www.monsterindia.com/data-science-jobs-3.html\n",
      "https://www.monsterindia.com/data-science-jobs-4.html\n",
      "https://www.monsterindia.com/data-science-jobs-5.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(82, 7)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_url(url):\n",
    "    page = requests.get(url).text\n",
    "    bs = BeautifulSoup(page, 'html.parser')\n",
    "    return (bs)\n",
    "\n",
    "def get_parents(bs):\n",
    "    return (bs.findAll('div', {'class': 'jobwrap', 'type': 'tuple'}))\n",
    "\n",
    "def get_attributes(job):\n",
    "    title = job.find('span', {'class': 'title_in'}).text\n",
    "    company = job.find('a', {'class': 'jtxt orange'}).get('title')\n",
    "    location = job.find('div', {'class': 'jtxt jico ico1'}).text\n",
    "    exp = job.find('div', {'class': 'jtxt jico ico2'}).text\n",
    "    posted_at = job.find('div', {'class': 'job_optitem ico7',\n",
    "                                 'itemprop':'datePosted'}).text\n",
    "    skills = job.find('div', {'class': 'joblnk serachjoblnk'}).findAll('div')[4].get('title')\n",
    "    curr_job = {'title': title,\n",
    "                'company': company,\n",
    "                'location': location,\n",
    "                'experience': exp,\n",
    "                'posted_at': posted_at,\n",
    "                'skills': skills\n",
    "               }\n",
    "    return curr_job\n",
    "\n",
    "jobs_df = pd.DataFrame(columns=['title', 'company', 'location',\n",
    "                                'exp', 'posted_at', 'skills'])\n",
    "\n",
    "for i in range(1,6):\n",
    "    url = 'https://www.monsterindia.com/data-science-jobs-%d.html' % i\n",
    "    print(url)\n",
    "    bs = parse_url(url)\n",
    "    parents = get_parents(bs)\n",
    "    for job in parents:\n",
    "        try:\n",
    "            jobs_df = jobs_df.append(get_attributes(job), ignore_index=True)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "jobs_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = pd.read_csv('https://raw.githubusercontent.com/skathirmani/datasets/master/amazon_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PlaintextParser.from_string(amazon['reviewText'][0], Tokenizer(\"english\"))\n",
    "summarizer = LexRankSummarizer()\n",
    "sentences = summarizer(parser.document, 3)\n",
    "#sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.yahoo.com/movies/rss'\n",
    "page = requests.get(url).text\n",
    "bs = BeautifulSoup(page, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = bs.findAll('item')\n",
    "# len(parents) #output: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Read the url and convert that to BS object\n",
    "    bs = parse_url(article_link)\n",
    "    \n",
    "    ## Step 2: Get all the paragraph tags in a list using findAll('p')\n",
    "    paras = bs.findAll('p')\n",
    "    \n",
    "    ## Step 3: Loop through each paragraph and join the strings\n",
    "    \n",
    "    ## Step 4: Using lexrank summarizer to summarize the article in 3 sentences\n",
    "    \n",
    "    ## Step 5: Print the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = LexRankSummarizer()\n",
    "for item in parents:\n",
    "    title = item.find('title').text\n",
    "    desc = item.find('description').text\n",
    "    bs_desc = BeautifulSoup(desc, 'html.parser')\n",
    "    article_link = bs_desc.find('a').get('href')\n",
    "    \n",
    "    bs = parse_url(article_link)\n",
    "    ptags = bs.findAll('p')\n",
    "    article_content = '. '.join([para.text for para in ptags])\n",
    "    \n",
    "    parser = PlaintextParser.from_string(article_content,\n",
    "                                         Tokenizer(\"english\"))\n",
    "    sentences = summarizer(parser.document, 3)\n",
    "    #print ('------------TITLE---------------')\n",
    "    #print(title)\n",
    "    #print('------SUMMARY------')\n",
    "    #print(sentences)\n",
    "    #print('------------------END--------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "- DTM\n",
    "    - Sparse matrix\n",
    "    - High dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1122f145da0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract GoogleNews....bin.gz\n",
    "path = 'GoogleNews-vectors-negative300.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path,\n",
    "                                                        binary=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.get_vector('computer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('computer', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
